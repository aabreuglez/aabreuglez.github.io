<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="map[]"><title>Conceptos básicos de Spark</title><meta name=generator content="Hugo 0.72.0"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet type=text/css href=https://adrianabreu.github.io/css/main.css><link rel=stylesheet type=text/css href=https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700,800" rel=stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js></script><script src=https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js></script><![endif]--></head><body><div id=wrap><nav class="navbar navbar-default"><div class=container><div class=navbar-header><a class=navbar-brand href=https://adrianabreu.github.io><i class="fa fa-home"></i></a></div><div id=navbar><ul class="nav navbar-nav navbar-right"><li><a href=/>INICIO</a></li><li><a href=/blog/>BLOG</a></li><li><a href=/knowledge/>KNOWLEDGE</a></li><li><a href=/archive/>ARCHIVE</a></li></ul></div></div></nav><div class=container><div class=aa-blogpost><div class=aa-blogpost-header><div class=aa-blogpost-header_title><a href=/blog/2019-11-09-spark-concepts-basicos/>Conceptos básicos de Spark</a></div><div class=aa-blogpost-header_date>09-11-2019</div></div></div><div class=aa-blogpost-content><p><strong>Nota del autor</strong>: Todos los contenidos de este artículo son extractos del libro &ldquo;The Data Engineer&rsquo;s Guide to Apache Spark&rdquo; que puedes descargar desde la pagina de databricks: <a href=https://databricks.com/lp/ebook/data-engineer-spark-guide>https://databricks.com/lp/ebook/data-engineer-spark-guide</a></p><h2 id=preludio>Preludio:</h2><h3 id=cluster>Cluster:</h3><p>Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</p><h3 id=spark-session>Spark Session:</h3><p>Es el objeto que tenemos para controlar nuestra aplicación de Spark. En Scala la variable &ldquo;spark&rdquo; representa este valor. Podemos generar esta sesión de SparkSession a traves de un &ldquo;builder&rdquo;. (Aquí podremos inyectar configuraciones, etcétera).</p><p><strong>Nota</strong>: Spark Session apareció como estándar en Spark 2.0, antes de esto se utilizaba un &ldquo;Spark Context&rdquo; que nos permitía solamente crear RDD&rsquo;s. Si queríamos hacer algo con sql había que instanciar un sqlContext, para hive un hiveContext&mldr; Para más información, este artículo lo describe muy bien: <a href=https://medium.com/@achilleus/spark-session-10d0d66d1d24>https://medium.com/@achilleus/spark-session-10d0d66d1d24</a></p><h2 id=conceptos-de-tipos-de-datos>Conceptos de tipos de datos:</h2><h3 id=dataframe>DataFrame:</h3><p>Es una representación de una tabla con filas y columnas. Podemos encontrar las columnas que contiene un dataframe y el tipo de las mismas en la propiedad schema. La ventaja de un dataframe es que puede estar distribuido a lo largo de decenas de máquinas. Es la forma más sencilla de trabajar con datos en Spark y tiene la ventaja de contar con un optimizador (Catalyst).</p><h3 id=datasets>Datasets:</h3><p>Son otra representación de datos pero tipados. Como &ldquo;truco&rdquo; un dataframe es un dataset de un tipo abstracto llamado &ldquo;Row&rdquo;.</p><h3 id=rdd>RDD:</h3><p>Son la primera aproximación que se hizo para computar datos de forma distribuida. De hecho se puede acceder al rdd de un dataframe. Se desrecomienda su uso por su &ldquo;fragilidad&rdquo; y porque la api de DataFrame ya ha igualado a la de RDD.</p><h2 id=conceptos-de-operaciones>Conceptos de operaciones:</h2><h3 id=particiones>Particiones:</h3><p>A diferencia de una partición de Hive (una carpeta con datos que podemos &ldquo;esquivar&rdquo; al hacer consultas) estas particiones son trozos de datos. Una serie de filas que residen en la misma máquina fisica. Para ajustar el numero de particiones disponemos de los métodos repartition y coalesce. La información sobre el impacto que tiene esto puede ver en Conceptos de Ejecución -> Tareas.</p><h3 id=transformaciones>Transformaciones:</h3><p>Son las instrucciones que podemos utilizar para modificar un dataframe. Una transformación no da ningún resultado ya que se evalúa mediante &ldquo;lazy evaluation&rdquo;, es decir, puedes encadenar operaciones que hasta que no ocurra una &ldquo;acción&rdquo;, estas transformaciones no se computan. De esta forma, Spark es capaz de organizar las transformaciones para dar el mejor plan de ejecución físico posible y ahorrar la mayor cantidad de &ldquo;shuffling&rdquo; (intercambiar datos entre nodos).</p><h3 id=acciones>Acciones:</h3><p>Son las instrucciones que hacen que se computen las transformaciones previas. La más simple puede ser un count. Pero son acciones cosas del tipo: un show para mostrar los datos, un write para escribir un output, un collect para devolver los datos al nodo driver.</p><h2 id=conceptos-de-ejecución>Conceptos de ejecución:</h2><h3 id=jobs>Jobs:</h3><p>Una acción desencadena un job para computar las operaciones. No es mas que una unidad de organización lógica.</p><h3 id=stages>Stages:</h3><p>Son &ldquo;pasos&rdquo; en una ejecución, basicamente agrupan tareas y resolven dependencias entre sí.</p><h3 id=tasks>Tasks:</h3><p>Es la unidad de ejecución de cualquier worker y la que trabaja con los datos directamente haciendo uso de los recursos. Esta directamente relacionada con el concepto de partición.</p><h2 id=extra>Extra</h2><p>En la web de <a href=https://supergloo.com/spark/spark-fair-scheduler-example/>https://supergloo.com/spark/spark-fair-scheduler-example/</a> se representa perfectamente la jerarquía entre operaciones y ejecución.</p><p><img src=https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_405,h_303/https://supergloo.com/wp-content/uploads/2017/09/spark-fair-scheduler.jpg alt></p></div></div><div class=aa-navigator><div class=row><div class="col-xs-2 col-lg-3" style=text-align:right><a href=https://adrianabreu.github.io/blog/2020-02-24-azure-functions-service-bus-limits/><i class="fa fa-2x fa-arrow-left" aria-hidden=true></i></a></div><div class="col-xs-8 col-lg-6 center"></div><div class="col-xs-2 col-lg-3"><a class="btn-floating btn-large paginate-button" href=https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/><i class="fa fa-2x fa-arrow-right" aria-hidden=true></i></a></div></div></div></div></div></body></html>