<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="map[]"><title>Empezando en Spark con Docker</title><meta name=generator content="Hugo 0.68.3"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet type=text/css href=https://adrianabreu.github.io/css/main.css><link rel=stylesheet type=text/css href=https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700,800" rel=stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js></script><script src=https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js></script><![endif]--></head><body><div id=wrap><nav class="navbar navbar-default"><div class=container><div class=navbar-header><a class=navbar-brand href=https://adrianabreu.github.io><i class="fa fa-home"></i></a></div><div id=navbar><ul class="nav navbar-nav navbar-right"><li><a href=/>INICIO</a></li><li><a href=/blog/>BLOG</a></li><li><a href=/knowledge/>KNOWLEDGE</a></li><li><a href=/archive/>ARCHIVE</a></li></ul></div></div></nav><div class=container><div class=aa-blogpost><div class=aa-blogpost-header><div class=aa-blogpost-header_title><a href=/blog/2019-11-10-empezando-en-spark-con-docker/>Empezando en Spark con Docker</a></div><div class=aa-blogpost-header_date>09-11-2019</div></div></div><div class=aa-blogpost-content><p>A pesar de haber leído guías tan buenas como:</p><p><a href=https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b>https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b</a></p><p><a href=https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597>https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597</a></p><p>Se me ha hecho cuesta arriba el poder conectar un notebook de jupyter y utilizar Scala. Entre configurar el apache toree para poder usar scala en los notebooks y algún error luego en spark al usarlo desde IntelliJ, me he acabado rindiendo.</p><p><em>Nota del autor:</em> Como disclaimer esto ocurre probablemente porque estoy en Manjaro y mi version de Scala es incompatible. Esta clase de problemas en su día las solucionaba fijando una versión, sin embargo, creo que teniendo una herramienta tan potente como es Docker estoy reinventando la rueda par un problema ya resuelto. Además de que voy a probarlo también en un windows para ver que es una solución agnóstical SO.</p><p>No voy a profundizar en nada de docker, simplemente podemos verlo como una &ldquo;virtualización ligera&rdquo; que se alimenta de imágenes para ejecutar procesos. Para el caso de configurar Spark con Jupyter podemos utilizar esta imagen de docker para nuestro propósito: <a href=https://hub.docker.com/r/jupyter/all-spark-notebook/>https://hub.docker.com/r/jupyter/all-spark-notebook/</a></p><p>Tras ejecutar el comando <code>docker pull jupyter/all-spark-notebook</code> y descargar los 2 GB de imagen:</p><img src=/images/spark-sample/pullingimage.png class=img-responsive><p>Ya solo nos quedaría arrancar el proceso. Para esto tenemos que mapear tres puertos del entorno de docker a nuestra máquina. Esto se hace en el propio comando de arranque</p><p><code>docker run -p 8888:8888/tcp -p 4040:4040/tcp jupyter/all-spark-notebook</code></p><img src=/images/spark-sample/runwithports.png class=img-responsive><p>Si copiamos la url que nos aparece, haremos el token de forma automática y podremos acceder a los notebook y ejecutar comandos de scala:</p><img src=/images/spark-sample/jupyter.png class=img-responsive><p>Y además en el puerto 4040 tendremos la spark ui para revisar nuestras ejecuciones:</p><img src=/images/spark-sample/sparkui.png class=img-responsive><p>Con esto, ya podemos practicar con Spark sin afectar a la configuración que tengamos en nuestra máquina.</p><h2 id=extra>Extra</h2><p>De forma opcional, vamos a montar un directorio en docker y vamos a ejecutar un job en Spark utilizando el comando spark-submit.</p><p>Lo primero, es que si estamos en Windows debemos compartir nuestro disco con docker, simplemente vamos a Settings -> Shared Drivers y habilitamos el disco.</p><img src=/images/spark-sample/sharingdrives.png class=img-responsive><p>Ahora necesitamos montar el directorio y esto lo haremos al arrancar la imagen. El comando sería:</p><p><code>docker run -p 8888:8888/tcp -p 4040:4040/tcp -v c:/spark-demo:/data jupyter/all-spark-notebook</code></p><p>Y en el notebook de jupyter abrimos una terminal para validarlo:</p><img src=/images/spark-sample/mounttable.png class=img-responsive><p>Ahora que ya tenemos esto, la idea es ver como sería un entorno &ldquo;productivo&rdquo; de Spark, más allá de los notebooks y mandar un spark job utilizando el punto de entrada de spark submit.</p><p>Nuestro programa no hará gran cosa, solo se trata de una aplicación de Scala que genera 500 numeros y los cuenta según sean múltiplos de 3 o no. (No hay que centrarse en el código). La parte más importante es que necesitamos mandar un assembly jar. Es decir, un jar que contiene sus dependencias. Todo esto está documentado por Spark aquí: <a href=https://spark.apache.org/docs/latest/submitting-applications.html>https://spark.apache.org/docs/latest/submitting-applications.html</a></p><p>Para poder crear el assembly vamos a utilizar sbt y el plugin <a href=https://github.com/sbt/sbt-assembly>https://github.com/sbt/sbt-assembly</a></p><p>Para poder dejar el articulo más ligero os dejo un enlace al repositorio de github con el código.</p><p><a href=https://github.com/adrianabreu/spark-multiple-of-3>https://github.com/adrianabreu/spark-multiple-of-3</a></p><p>Simplemente tenemos que ejecutar el comando &ldquo;sbt assembly&rdquo; para generar un jar y este jar lo ponemos en la carpeta del docker.</p><p>Ahora utilizaremos el comando de spark-submit para mandar el job.</p><p><code>/usr/local/spark/bin/spark-submit --class "example.Application" multiple-of-3-assembly-1.0.0.jar</code></p><p>Et voilá!</p><img src=/images/spark-sample/sparksubmit.png class=img-responsive><h2 id=faq>FAQ:</h2><p>Si hay algún error con puertos y al intentar rearrancar la imagen de docker aparece que los puertos estan en uso, basta con hacer un docker ps -a copiar el container id y hacer un docker kill containerid.</p></div></div><div class=aa-navigator><div class=row><div class="col-xs-2 col-lg-3" style=text-align:right><a href=https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/><i class="fa fa-2x fa-arrow-left" aria-hidden=true></i></a></div><div class="col-xs-8 col-lg-6 center"></div><div class="col-xs-2 col-lg-3"><a class="btn-floating btn-large paginate-button" href=https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/><i class="fa fa-2x fa-arrow-right" aria-hidden=true></i></a></div></div></div></div></div></body></html>