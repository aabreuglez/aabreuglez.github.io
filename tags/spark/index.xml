<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark on Adrián Abreu</title><link>https://adrianabreu.github.io/tags/spark/</link><description>Recent content in Spark on Adrián Abreu</description><generator>Hugo -- gohugo.io</generator><language>es-ES</language><copyright>2017 Adrián Abreu powered by Hugo and Kiss Theme</copyright><lastBuildDate>Tue, 11 Aug 2020 18:52:32 +0000</lastBuildDate><atom:link href="https://adrianabreu.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark windows functions (I)</title><link>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</link><pubDate>Tue, 11 Aug 2020 18:52:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2020-08-11-spark-windows-functions/</guid><description>En analítica, es muy común hacer uso de las funciones de ventana para distintos cálculos. Hace poco me encontré con un pequeño problema cuya solución mejoró muchísimo al usar las funciones de ventana, demos un poco de contexto.
Tenemos una dimensión de usuarios donde los usuarios se van registrando con una fecha y tenemos una tabla de ventas donde tenemos las ventas globales para cada día
Y lo que queremos dar es una visión de cómo cada día evoluciona el programa, para ello se quiere que cada día estén tanto las ventas acumuladas como los registros acumulados.</description></item><item><title>Conceptos básicos de Spark</title><link>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-09-spark-concepts-basicos/</guid><description>Nota del autor: Todos los contenidos de este artículo son extractos del libro &amp;ldquo;The Data Engineer&amp;rsquo;s Guide to Apache Spark&amp;rdquo; que puedes descargar desde la pagina de databricks: https://databricks.com/lp/ebook/data-engineer-spark-guide
Preludio: Cluster: Un cluster no es más que un conjunto de máquinas trabajando de forma coordinada. Un cluster de Spark se compone de nodos. Uno actúa como DRIVER y es el punto de entrada para el código del usuario. Los otros actúan como EXECUTOR que seran los encargados de realizar las operaciones.</description></item><item><title>Empezando en Spark con Docker</title><link>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</link><pubDate>Sat, 09 Nov 2019 19:22:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-11-10-empezando-en-spark-con-docker/</guid><description>A pesar de haber leído guías tan buenas como:
https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b
https://medium.com/@singhpraveen2010/install-apache-spark-and-configure-with-jupyter-notebook-in-10-minutes-ae120ebca597
Se me ha hecho cuesta arriba el poder conectar un notebook de jupyter y utilizar Scala. Entre configurar el apache toree para poder usar scala en los notebooks y algún error luego en spark al usarlo desde IntelliJ, me he acabado rindiendo.
Nota del autor: Como disclaimer esto ocurre probablemente porque estoy en Manjaro y mi version de Scala es incompatible.</description></item><item><title>Correlated subqueries</title><link>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</link><pubDate>Thu, 26 Sep 2019 20:43:32 +0000</pubDate><guid>https://adrianabreu.github.io/blog/2019-09-26-correlated-subqueries/</guid><description>Llevo un par de meses viendo como la mayoría de esfuerzos en el proyecto en el que estoy se centran en evitar los joins en las distintas capas de análisis. Aprovechando las capacidades de spark se busca tener las estructuras muy desnormalizadas y se había &amp;ldquo;endemoniado&amp;rdquo; al join considerarlo perjudicial.
Tanto es así que llevo un par de días peleando con una pieza de código que me ha sorprendido. Partiendo de una tabla de hechos que agrupa datos para un periodo a hasta b, se quiere que se &amp;ldquo;colapsen&amp;rdquo; los datos de hace 14 días.</description></item></channel></rss>